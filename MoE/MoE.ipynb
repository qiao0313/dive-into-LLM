{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MoE\n",
    "\n",
    "MoE，全称为Mixed Expert Models，翻译过来就是混合专家模型。MoE 的一个显著优势是能够在远少于 Dense 模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，可以显著扩大模型或数据集的规模。特别是在预训练阶段，与 Dense 模型相比，混合专家模型通常能够更快地达到相同的质量水平。在混合专家语言模型中，大部分组件都与传统的 transformers 相同。\n",
    "\n",
    "MoE 基于 Transformer 架构，主要由两部分组成：\n",
    "- MoE 层：这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构。\n",
    "- 门控网络或路由: 这个部分用于决定哪些 token 被发送到哪个专家。例如，在下图中，“More”这个 token 可能被发送到第二个专家，而“Parameters”这个 token 被发送到第一个专家。有时，一个 token 甚至可以被发送到多个专家。token 的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/MoE_architecture.jpg\" alt=\"MoE architecture\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "总结来说，在混合专家模型 (MoE) 中，将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个路由器（或者叫门控网络）和若干数量的专家。\n",
    "\n",
    "MoE的优点如下：\n",
    "- 训练速度更快，效果更好。\n",
    "- 相同参数，推理成本低。\n",
    "- 扩展性好，允许模型在保持计算成本不变的情况下增加参数数量，这使得它能够扩展到非常大的模型规模，如万亿参数模型。\n",
    "- 多任务学习能力，MoE 在多任务学习中具备很好的新能。\n",
    "\n",
    "MoE的缺点如下：\n",
    "- 训练稳定性，MoE在训练过程中可能会遇到稳定性问题。\n",
    "- 通信成本，在分布式训练环境中，MoE 的专家路由机制可能会增加通信成本，尤其是在模型规模较大时。\n",
    "- 模型复杂性，MoE 的设计相对复杂，可能需要更多的工程努力来实现和优化。\n",
    "- 下游任务性能，MoE由于其稀疏性，使得在 Fine-tuning 过程中容易出现过拟合。\n",
    "\n",
    "主要介绍几个模块的实现：\n",
    "- self-attention 以及 multi-head self-attention 模块的实现\n",
    "- 稀疏混合专家代替单独的前馈神经网络\n",
    "- Top-k 门控和有噪声的 Top-k 门控\n",
    "\n",
    "实现代码参考 [makeMoE](https://github.com/AviSoori1x/makeMoE) 项目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-attention\n",
    "\n",
    "常规的 self-attention 实现方式使用的缩放点积自注意力，查询矩阵、键矩阵和值矩阵都来自相同的输入序列，同时为了确保自回归语言生成过程的完整性，特别是在纯解码器模型中，使用了一种因果自注意力，也叫因果掩码。它可以掩盖当前 token 所处位置之后的任何信息，从而引导模型只关注序列的前面部分。值得注意的是，稀疏混合专家模型并不局限于仅有解码器的 Transformer 架构。事实上，这一领域的许多重要的成果都是围绕 T5 架构展开的，T5 架构也包含了 Transformer 模型中的编码器和解码器组件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tril: \n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 首先先实现 self-attention 模块的工作原理代码\n",
    "# 创建一个 [batch_size, seq_len, hidden_dim] 的张量，命名为 x\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch_size, seq_len, n_embed = 4, 8, 32\n",
    "x = torch.randn(batch_size, seq_len, n_embed)\n",
    "\n",
    "# 接下来实现一个单头的 self-attention 模块的工作原理代码\n",
    "head_size = 16 # 每个头的维度\n",
    "key = nn.Linear(n_embed, head_size, bias=False)\n",
    "query = nn.Linear(n_embed, head_size, bias=False)\n",
    "value = nn.Linear(n_embed, head_size, bias=False)\n",
    "k = key(x)   # (4, 8, 16)\n",
    "q = query(x) # (4, 8, 16)\n",
    "weight = q @ k.transpose(-2, -1) # (4, 8, 16) @ (4, 16, 8) ---> (4, 8, 8)\n",
    "\n",
    "# 为了保证每个 token 只能关注到其自身以及前面几个 token，需要对 weight 进行 mask，即使用一个 [seq_len, seq_len] 的因果掩码\n",
    "tril = torch.tril(torch.ones(seq_len, seq_len))\n",
    "print(\"tril: \\n{}\".format(tril))\n",
    "weight = weight.masked_fill(tril == 0, float('-inf'))\n",
    "weight = F.softmax(weight, dim=-1) # (4, 8, 8)\n",
    "\n",
    "v = value(x) # (4, 8, 16)\n",
    "out = tril @ v # (4, 8, 8) @ (4, 8, 16) -> (4, 8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接来下将相关模块整合成一个函数，方便调用\n",
    "\n",
    "# 单头的自注意力机制\n",
    "class OneHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, head_size, seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "        k = self.key(x)   # (batch_size, seq_len, head_size)\n",
    "        q = self.query(x) # (batch_size, seq_len, head_size)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        weight = q @ k.transpose(-2,-1) * n_embed ** -0.5 # (batch_size, seq_len, head_size) @ (batch_size, head_size, seq_len) -> (batch_size, seq_len, seq_len)\n",
    "        weight = weight.masked_fill(self.tril[:seq_len, :seq_len] == 0, float('-inf')) # (batch_size, seq_len, seq_len)\n",
    "        weight = F.softmax(weight, dim=-1) # (batch_size, seq_len, seq_len)\n",
    "        weight = self.dropout(weight)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (batch_size, seq_len, head_size)\n",
    "        out = weight @ v # (batch_size, seq_len, seq_len) @ (batch_size, seq_len, head_size) -> (batch_size, seq_len, head_size)\n",
    "        return out\n",
    "\n",
    "# 基于单头的自注意力机制实现多头的自注意力机制\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_heads, head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([OneHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专家模块 - 多层感知器\n",
    "\n",
    "在稀疏混合专家架构中，每个 Transformer 区块内的自注意力机制保持不变。不过，每个区块的结构发生了巨大的变化：标准的前馈神经网络被多个稀疏激活的前馈网络（即专家网络）所取代。所谓「稀疏激活」，是指序列中的每个 token 只被分配给有限数量的专家（通常是一个或两个）。\n",
    "\n",
    "这有助于提高训练和推理速度，因为每次前向传递都会激活少数专家。不过，所有专家都必须存在 GPU 内存中，因此当参数总数达到数千亿甚至数万亿时，就会产生部署方面的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 专家模块的实现和FFN模块的实现相同\n",
    "class Expert(nn.Module):\n",
    "    \"\"\" An MLP is a simple linear layer followed by a non-linearity i.e. each Expert \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-k 门控\n",
    "\n",
    "门控网络，也称为路由，确定哪个专家网络接收来自多头注意力的 token 的输出。假设有 4 个专家，token 需要被路由到前 2 个专家中。首先需要通过线性层将 token 输入到门控网络中。该层将对应于（batch_size，seq_len，n_embed）的输入张量从（4，8，32）维度，投影到对应于（batch_size，seq_len，num_expert）的新形状：（4、8，4）。其中 n_embed 是输入的通道维度，num_experts 是专家网络的计数。然后，沿最后一个维度，找出最大的前两个值及其相应的索引。\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/top_k_gating.jpg\" alt=\"top-k gating\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.0700,  0.7206],\n",
       "          [ 0.2494, -0.0838],\n",
       "          [ 1.2022,  0.5802],\n",
       "          [ 0.8623,  0.6392],\n",
       "          [ 0.3154,  0.0610],\n",
       "          [ 0.8664,  0.6319],\n",
       "          [ 0.5692,  0.0469],\n",
       "          [ 1.3120, -0.3133]],\n",
       " \n",
       "         [[ 1.2228, -0.0321],\n",
       "          [ 1.1416,  0.3027],\n",
       "          [ 0.5253,  0.4374],\n",
       "          [ 0.1580, -0.0446],\n",
       "          [ 0.3139,  0.2930],\n",
       "          [ 0.3529,  0.2312],\n",
       "          [ 1.4150,  0.2912],\n",
       "          [ 0.5945,  0.1327]],\n",
       " \n",
       "         [[ 0.5750, -0.0629],\n",
       "          [ 0.6928,  0.2333],\n",
       "          [ 0.6365,  0.2649],\n",
       "          [ 0.4032,  0.1236],\n",
       "          [ 0.8245, -0.1826],\n",
       "          [ 1.3292,  0.2458],\n",
       "          [-0.0589, -0.0794],\n",
       "          [ 0.8956,  0.6806]],\n",
       " \n",
       "         [[ 0.1375,  0.0218],\n",
       "          [ 0.4306,  0.0555],\n",
       "          [ 1.3460,  0.9864],\n",
       "          [ 0.9852,  0.1215],\n",
       "          [ 1.0872,  0.1047],\n",
       "          [ 0.1002, -0.1142],\n",
       "          [ 1.3141, -0.5620],\n",
       "          [ 0.9606, -0.1521]]], grad_fn=<TopkBackward0>),\n",
       " tensor([[[3, 2],\n",
       "          [2, 0],\n",
       "          [3, 1],\n",
       "          [1, 0],\n",
       "          [1, 2],\n",
       "          [3, 2],\n",
       "          [1, 3],\n",
       "          [0, 1]],\n",
       " \n",
       "         [[2, 3],\n",
       "          [1, 3],\n",
       "          [2, 0],\n",
       "          [2, 1],\n",
       "          [2, 0],\n",
       "          [1, 3],\n",
       "          [2, 0],\n",
       "          [0, 1]],\n",
       " \n",
       "         [[1, 0],\n",
       "          [2, 1],\n",
       "          [2, 0],\n",
       "          [3, 2],\n",
       "          [1, 2],\n",
       "          [1, 3],\n",
       "          [2, 1],\n",
       "          [0, 3]],\n",
       " \n",
       "         [[1, 2],\n",
       "          [1, 2],\n",
       "          [2, 3],\n",
       "          [1, 2],\n",
       "          [3, 2],\n",
       "          [3, 2],\n",
       "          [1, 3],\n",
       "          [2, 1]]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过一个简单的例子来理解 top-k 门控机制\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "n_embed = 32\n",
    "\n",
    "# 假如经过 multi-head self attention 之后，得到一个 (4, 8, 32) 的张量\n",
    "mh_output = torch.randn(batch_size, seq_len, n_embed)\n",
    "\n",
    "topkgate_linear = nn.Linear(n_embed, num_experts) # nn.Linear(32, 4)\n",
    "\n",
    "logits = topkgate_linear(mh_output)\n",
    "top_k_logits, top_k_indices = logits.topk(top_k, dim=-1)  # Get top-k experts\n",
    "top_k_logits, top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过仅保留沿最后一个维度进行比较的前 k 大的值，来获得稀疏门控的输出。用负无穷值填充其余部分，在使用 softmax 激活函数。负无穷会被映射至零，而最大的前两个值会更加突出，且和为 1。要求和为 1 是为了对专家输出的内容进行加权。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   -inf,  0.2479,    -inf,  0.9578],\n",
       "         [-0.4220, -0.4273,    -inf,    -inf],\n",
       "         [ 0.1390,    -inf,    -inf,  0.1366],\n",
       "         [   -inf,  0.2133,    -inf,  0.2477],\n",
       "         [   -inf,  0.4563,  0.7880,    -inf],\n",
       "         [ 0.1496,    -inf,    -inf, -0.0354],\n",
       "         [-0.0292,    -inf,  1.0569,    -inf],\n",
       "         [ 0.5379,    -inf,  0.1495,    -inf]],\n",
       "\n",
       "        [[ 0.5540,    -inf,  0.2338,    -inf],\n",
       "         [ 0.2380,    -inf,  0.8278,    -inf],\n",
       "         [   -inf,  0.1439,  0.6391,    -inf],\n",
       "         [ 0.2268,  0.4964,    -inf,    -inf],\n",
       "         [ 0.1168, -0.1425,    -inf,    -inf],\n",
       "         [ 0.3889,  0.4419,    -inf,    -inf],\n",
       "         [ 1.0024,    -inf,  0.0241,    -inf],\n",
       "         [   -inf, -0.1975,  0.5915,    -inf]],\n",
       "\n",
       "        [[ 0.6596,    -inf,    -inf,  0.7077],\n",
       "         [   -inf,    -inf,  0.6383,  0.7035],\n",
       "         [   -inf,    -inf,  0.2839,  1.1395],\n",
       "         [ 0.2079,    -inf,  0.5646,    -inf],\n",
       "         [   -inf,  0.0644,  0.8486,    -inf],\n",
       "         [ 0.1942,    -inf,    -inf, -0.0916],\n",
       "         [   -inf,    -inf,  1.0266,  0.3353],\n",
       "         [ 0.2811,    -inf,    -inf,  0.4230]],\n",
       "\n",
       "        [[ 0.6236,    -inf,    -inf,  0.8929],\n",
       "         [-0.0068,    -inf,  0.3225,    -inf],\n",
       "         [   -inf,    -inf, -0.1321,  0.5988],\n",
       "         [   -inf,  0.1115,    -inf,  0.5310],\n",
       "         [   -inf,  0.1746,  0.6859,    -inf],\n",
       "         [   -inf,  0.1188,  0.9117,    -inf],\n",
       "         [ 0.4967,    -inf,  0.4524,    -inf],\n",
       "         [ 0.4938,  0.4175,    -inf,    -inf]]], grad_fn=<ScatterBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.full_like(logits, float('-inf')) \n",
    "sparse_logits = zeros.scatter(-1, top_k_indices, top_k_logits)\n",
    "sparse_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.3296, 0.0000, 0.6704],\n",
       "         [0.5013, 0.4987, 0.0000, 0.0000],\n",
       "         [0.5006, 0.0000, 0.0000, 0.4994],\n",
       "         [0.0000, 0.4914, 0.0000, 0.5086],\n",
       "         [0.0000, 0.4178, 0.5822, 0.0000],\n",
       "         [0.5461, 0.0000, 0.0000, 0.4539],\n",
       "         [0.2523, 0.0000, 0.7477, 0.0000],\n",
       "         [0.5959, 0.0000, 0.4041, 0.0000]],\n",
       "\n",
       "        [[0.5794, 0.0000, 0.4206, 0.0000],\n",
       "         [0.3567, 0.0000, 0.6433, 0.0000],\n",
       "         [0.0000, 0.3787, 0.6213, 0.0000],\n",
       "         [0.4330, 0.5670, 0.0000, 0.0000],\n",
       "         [0.5645, 0.4355, 0.0000, 0.0000],\n",
       "         [0.4868, 0.5132, 0.0000, 0.0000],\n",
       "         [0.7268, 0.0000, 0.2732, 0.0000],\n",
       "         [0.0000, 0.3124, 0.6876, 0.0000]],\n",
       "\n",
       "        [[0.4880, 0.0000, 0.0000, 0.5120],\n",
       "         [0.0000, 0.0000, 0.4837, 0.5163],\n",
       "         [0.0000, 0.0000, 0.2983, 0.7017],\n",
       "         [0.4118, 0.0000, 0.5882, 0.0000],\n",
       "         [0.0000, 0.3134, 0.6866, 0.0000],\n",
       "         [0.5710, 0.0000, 0.0000, 0.4290],\n",
       "         [0.0000, 0.0000, 0.6663, 0.3337],\n",
       "         [0.4646, 0.0000, 0.0000, 0.5354]],\n",
       "\n",
       "        [[0.4331, 0.0000, 0.0000, 0.5669],\n",
       "         [0.4184, 0.0000, 0.5816, 0.0000],\n",
       "         [0.0000, 0.0000, 0.3250, 0.6750],\n",
       "         [0.0000, 0.3966, 0.0000, 0.6034],\n",
       "         [0.0000, 0.3749, 0.6251, 0.0000],\n",
       "         [0.0000, 0.3116, 0.6884, 0.0000],\n",
       "         [0.5111, 0.0000, 0.4889, 0.0000],\n",
       "         [0.5191, 0.4809, 0.0000, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_output= F.softmax(sparse_logits, dim=-1)\n",
    "gating_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 top-k 门控机制整理成一个函数\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(TopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        self.linear =nn.Linear(n_embed, num_experts)\n",
    "    \n",
    "    def forward(self, mh_output):\n",
    "        # mh_ouput is the output tensor from multi-head self attention block\n",
    "        logits = self.linear(mh_output)\n",
    "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 4]),\n",
       " tensor([[[0.4262, 0.0000, 0.0000, 0.5738],\n",
       "          [0.0000, 0.3942, 0.0000, 0.6058],\n",
       "          [0.0000, 0.5768, 0.4232, 0.0000],\n",
       "          [0.3226, 0.0000, 0.0000, 0.6774],\n",
       "          [0.3806, 0.0000, 0.6194, 0.0000],\n",
       "          [0.7507, 0.0000, 0.2493, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4478, 0.5522],\n",
       "          [0.0000, 0.0000, 0.3864, 0.6136]],\n",
       " \n",
       "         [[0.2937, 0.7063, 0.0000, 0.0000],\n",
       "          [0.3889, 0.6111, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5416, 0.4584, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4122, 0.5878],\n",
       "          [0.3586, 0.0000, 0.6414, 0.0000],\n",
       "          [0.2137, 0.0000, 0.0000, 0.7863],\n",
       "          [0.3996, 0.0000, 0.6004, 0.0000],\n",
       "          [0.0000, 0.6880, 0.0000, 0.3120]],\n",
       " \n",
       "         [[0.5106, 0.4894, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5588, 0.4412, 0.0000],\n",
       "          [0.0000, 0.3291, 0.6709, 0.0000],\n",
       "          [0.0000, 0.5787, 0.0000, 0.4213],\n",
       "          [0.5344, 0.0000, 0.0000, 0.4656],\n",
       "          [0.3891, 0.0000, 0.6109, 0.0000],\n",
       "          [0.0000, 0.2467, 0.7533, 0.0000],\n",
       "          [0.6822, 0.0000, 0.3178, 0.0000]],\n",
       " \n",
       "         [[0.4843, 0.0000, 0.0000, 0.5157],\n",
       "          [0.5650, 0.0000, 0.0000, 0.4350],\n",
       "          [0.0000, 0.0000, 0.7359, 0.2641],\n",
       "          [0.2733, 0.0000, 0.0000, 0.7267],\n",
       "          [0.3936, 0.6064, 0.0000, 0.0000],\n",
       "          [0.4821, 0.5179, 0.0000, 0.0000],\n",
       "          [0.3116, 0.0000, 0.6884, 0.0000],\n",
       "          [0.0000, 0.8486, 0.1514, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[3, 0],\n",
       "          [3, 1],\n",
       "          [1, 2],\n",
       "          [3, 0],\n",
       "          [2, 0],\n",
       "          [0, 2],\n",
       "          [3, 2],\n",
       "          [3, 2]],\n",
       " \n",
       "         [[1, 0],\n",
       "          [1, 0],\n",
       "          [1, 2],\n",
       "          [3, 2],\n",
       "          [2, 0],\n",
       "          [3, 0],\n",
       "          [2, 0],\n",
       "          [1, 3]],\n",
       " \n",
       "         [[0, 1],\n",
       "          [1, 2],\n",
       "          [2, 1],\n",
       "          [1, 3],\n",
       "          [0, 3],\n",
       "          [2, 0],\n",
       "          [2, 1],\n",
       "          [0, 2]],\n",
       " \n",
       "         [[3, 0],\n",
       "          [0, 3],\n",
       "          [2, 3],\n",
       "          [3, 0],\n",
       "          [1, 0],\n",
       "          [1, 0],\n",
       "          [2, 0],\n",
       "          [1, 2]]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试用例\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "n_embed = 32\n",
    "\n",
    "mh_output = torch.randn(batch_size, seq_len, n_embed)\n",
    "top_k_gate = TopkRouter(n_embed, num_experts, top_k)\n",
    "gating_output, indices = top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有噪声的 Top-k 门控 —— 实现负载平衡\n",
    "\n",
    "有噪声的 Top-k 门控机制是训练 MoE 模型的一个重要工具。从本质上讲，不会希望所有的 token 都发送给同一组「受欢迎」的专家网络。人们需要的是能在开发和探索之间取得良好平衡。为此，为了负载平衡，从门控的线性层向 logits 激活函数添加标准正态噪声是有帮助的，这使训练更有效率。\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"Images/noised_top_k_gating.jpg\" alt=\"noised top-k gating\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        # layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        # mh_output is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "\n",
    "        # Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        # Adding scaled unit gaussian noise to the logits\n",
    "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 8]),\n",
       " tensor([[[0.0000, 0.5102, 0.0000, 0.0000, 0.0000, 0.4898, 0.0000, 0.0000],\n",
       "          [0.4597, 0.0000, 0.0000, 0.5403, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.5533, 0.0000, 0.4467, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5424, 0.0000, 0.4576, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0710, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9290],\n",
       "          [0.0000, 0.0000, 0.9008, 0.0000, 0.0000, 0.0992, 0.0000, 0.0000],\n",
       "          [0.7085, 0.0000, 0.0000, 0.0000, 0.2915, 0.0000, 0.0000, 0.0000],\n",
       "          [0.3907, 0.0000, 0.6093, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.3858, 0.0000, 0.0000, 0.6142, 0.0000, 0.0000],\n",
       "          [0.5314, 0.0000, 0.0000, 0.4686, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2911, 0.0000, 0.7089],\n",
       "          [0.0000, 0.0000, 0.0000, 0.6592, 0.0000, 0.0000, 0.3408, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.4964, 0.0000, 0.0000, 0.5036, 0.0000],\n",
       "          [0.4978, 0.0000, 0.5022, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5174, 0.0000, 0.0000, 0.0000, 0.4826, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6100, 0.0000, 0.3900]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.9672, 0.0000, 0.0000, 0.0000, 0.0328],\n",
       "          [0.5358, 0.0000, 0.4642, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3875, 0.0000, 0.0000, 0.6125, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6422, 0.0000, 0.3578],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6100, 0.3900],\n",
       "          [0.0000, 0.0000, 0.4177, 0.5823, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.4430, 0.0000, 0.5570, 0.0000, 0.0000],\n",
       "          [0.5847, 0.0000, 0.0000, 0.4153, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.7918, 0.0000, 0.0000, 0.0000, 0.0000, 0.2082],\n",
       "          [0.0000, 0.4961, 0.0000, 0.5039, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5391, 0.4609],\n",
       "          [0.3363, 0.0000, 0.6637, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4085, 0.0000, 0.0000, 0.5915, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4069, 0.5931, 0.0000],\n",
       "          [0.0000, 0.0000, 0.8362, 0.0000, 0.0000, 0.1638, 0.0000, 0.0000],\n",
       "          [0.2447, 0.0000, 0.7553, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[1, 5],\n",
       "          [3, 0],\n",
       "          [3, 5],\n",
       "          [1, 3],\n",
       "          [7, 1],\n",
       "          [2, 5],\n",
       "          [0, 4],\n",
       "          [2, 0]],\n",
       " \n",
       "         [[5, 2],\n",
       "          [0, 3],\n",
       "          [7, 5],\n",
       "          [3, 6],\n",
       "          [6, 3],\n",
       "          [2, 0],\n",
       "          [0, 4],\n",
       "          [5, 7]],\n",
       " \n",
       "         [[3, 7],\n",
       "          [0, 2],\n",
       "          [5, 2],\n",
       "          [5, 7],\n",
       "          [6, 7],\n",
       "          [3, 2],\n",
       "          [5, 3],\n",
       "          [0, 3]],\n",
       " \n",
       "         [[2, 7],\n",
       "          [3, 1],\n",
       "          [6, 7],\n",
       "          [2, 0],\n",
       "          [5, 2],\n",
       "          [6, 5],\n",
       "          [2, 5],\n",
       "          [2, 0]]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试用例\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "n_embed = 32\n",
    "\n",
    "mh_output = torch.randn(batch_size, seq_len, n_embed)\n",
    "noisy_top_k_gate = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "gating_output, indices = noisy_top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 稀疏化的混合专家模块\n",
    "\n",
    "在获得门控网络的输出结果之后，对于给定的 token，将前 k 个值选择性地与来自相应的前 k 个专家的输出相乘。这种选择性乘法的结果是一个加权和，该加权和构成 SparseMoe 模块的输出。这个过程的关键和难点是避免不必要的乘法运算，只为前 k 名专家进行正向转播。为每个专家执行前向传播将破坏使用稀疏 MoE 的目的，因为这个过程将不再是稀疏的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k, dropout):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed, dropout) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final output: torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "# 测试用例\n",
    "num_experts = 8\n",
    "top_k = 2\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "n_embed = 32\n",
    "dropout = 0.1\n",
    "\n",
    "mh_output = torch.randn(batch_size, seq_len, n_embed)\n",
    "sparse_moe = SparseMoE(n_embed, num_experts, top_k, dropout)\n",
    "final_output = sparse_moe(mh_output)\n",
    "print(\"Shape of the final output:\", final_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模块整合\n",
    "\n",
    "将多头自注意力和稀疏混合专家相结合，形成稀疏混合专家 Transformer 块。就像在 vanilla transformer 块中一样，也要使用残差以确保训练稳定，并避免梯度消失等问题。此外，要采用层归一化来进一步稳定学习过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Mixture of Experts Transformer block: communication followed by computation (multi-head self attention + SparseMoE) \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head, num_experts, top_k, dropout):\n",
    "        # n_embed: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.smoe = SparseMoE(n_embed, num_experts, top_k, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.smoe(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，将所有内容整合在一起，形成稀疏混合专家语言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoELanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embed, block_size, n_head, n_layer, num_experts, top_k):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embed = n_embed\n",
    "        self.block_size = block_size\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head, num_experts=num_experts, top_k=top_k) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        batch_size, seq_len = idx.shape\n",
    "\n",
    "        # idx and targets are both (batch_size, seq_len) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (batch_size, seq_len, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(seq_len)) # (seq_len, n_embed)\n",
    "        x = tok_emb + pos_emb # (batch_size, seq_len, n_embed)\n",
    "        x = self.blocks(x) # (batch_size, seq_len, n_embed)\n",
    "        x = self.ln_f(x) # (batch_size, seq_len, n_embed)\n",
    "        logits = self.lm_head(x) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, n_embed = logits.shape\n",
    "            logits = logits.view(batch_size*seq_len, n_embed)\n",
    "            targets = targets.view(batch_size*seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
